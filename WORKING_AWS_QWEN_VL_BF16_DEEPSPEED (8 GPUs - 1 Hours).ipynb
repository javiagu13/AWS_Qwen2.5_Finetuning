{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w75hnjziSu4R"
   },
   "source": [
    "# Qwen 2.5 VL Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training 7B Qwen 2.5 VL in bf16 format using AWS p4d and p4de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuPgOdttS8sR"
   },
   "source": [
    "**1-Library Installation and environment setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WteJPGWeIK6m",
    "outputId": "c1299c7d-5e41-4631-9dd2-3f82fa9f07c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git config --global credential.helper store\n",
    "!pip install huggingface_hub\n",
    "!huggingface-cli login --token YOUR_HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AWS CLI if not already installed\n",
    "!pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS CLI with your credentials\n",
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sts get-caller-identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip \n",
    "!pip install sagemaker transformers datasets peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Define Training Datasets inside of your S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Specify your custom bucket name\n",
    "bucket_name = \"vlm-training-s3\"  # Same as used in upload\n",
    "s3_prefix = \"vlm-training-data\"  # Same as used in upload\n",
    "chosen_schema = \"qwen\"\n",
    "\n",
    "# And update the file paths accordingly:\n",
    "train_s3_path = f\"{s3_prefix}/train/{chosen_schema}_train_set.tsv\"\n",
    "validation_s3_path = f\"{s3_prefix}/validation/{chosen_schema}_validation_set.tsv\"\n",
    "test_s3_path = f\"{s3_prefix}/test/{chosen_schema}_test_set.tsv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-Main Training File Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all this is executed in a docker container and we cannot execute cell by cell, therefore, this code creates different codes in the current folder to be able to execute all of them inside of the AWS container to communicate with the cluster and the A100 8 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** Please, in this cell the main thing you need to change is the model name. It should correspond to the same model name you used for the data preprocessing. So for instance in the case of LG exaone, the huggingface name is: **LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct**. The second thing you will need to do is to ensure that the **.tsv** name matches the same as described in the previous cell. For example: **exaone_train_set.tsv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Secondary Note:** If you wonder why the preprocessing code is included inside of this code, there is a good reason. Since we cannot control the execution and ensure that tokenization was done right due to executing in a docker container, we preprocess inside of the container first to check all the tokenization works properly prior to passing it to the model. This way we can see what is happening inside of the docker container, otherwise, it would be very hard to debug since we cannot run cell by cell as in a normal notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "!explorer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_deploy_huggingface.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_deploy_huggingface.py\n",
    "import os\n",
    "import argparse\n",
    "import subprocess\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "import deepspeed\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    "    AutoModelForVision2Seq, \n",
    "    AutoProcessor,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def list_dir_contents(path):\n",
    "    \"\"\"\n",
    "    Recursively lists all files and directories within the given path,\n",
    "    along with their sizes.\n",
    "    \"\"\"\n",
    "    print(f\"\\nContents of '{path}':\")\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        level = root.replace(path, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            file_path = os.path.join(root, f)\n",
    "            size = os.path.getsize(file_path) / 1e6  # Size in MB\n",
    "            print(f\"{sub_indent}{f} - {size:.2f} MB\")\n",
    "\n",
    "def download_images_from_s3(bucket, prefix, local_dir):\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        file_key = obj[\"Key\"]\n",
    "        local_file_path = os.path.join(local_dir, os.path.basename(file_key))\n",
    "        s3.download_file(bucket, file_key, local_file_path)\n",
    "        print(f\"Downloaded {file_key} to {local_file_path}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"Per-device training batch size\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=4, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=16, help=\"Gradient accumulation steps\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=4000, help=\"Maximum sequence length\")\n",
    "\n",
    "    # Environment variables set by SageMaker\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "    parser.add_argument(\"--train_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--validation_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "\n",
    "    # **Add the following line to accept --local_rank**\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Local rank for distributed training\")\n",
    "    parser.add_argument(\"--images_path\", type=str, default=\"/opt/ml/input/data/images\", help=\"Local path for images\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ----- Removed CSV/S3-based data loading as per tutorial instructions -----\n",
    "    # (The following block was removed:)\n",
    "    #   - Printing image directory structure and sample paths.\n",
    "    #   - Downloading images from S3.\n",
    "\n",
    "    # Initialize processor, tokenizer and model\n",
    "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "    tokenizer = processor.tokenizer  # Define tokenizer from the processor\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        token='YOUR_HUGGINGFACE_TOKEN'\n",
    "    )\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    with open('./deepspeed_config.json', 'r') as config_file:\n",
    "        deepspeed_config = json.load(config_file)\n",
    "\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # ----------- Load and Format the ChartQA Dataset from Hugging Face -----------\n",
    "    # The tutorial uses the HuggingFaceM4/ChartQA dataset.\n",
    "    dataset_id = \"HuggingFaceM4/ChartQA\"\n",
    "    raw_train_dataset, raw_eval_dataset, raw_test_dataset = load_dataset(dataset_id, split=[\"train[:10%]\", \"val[:10%]\", \"test[:10%]\"])\n",
    "    \n",
    "    # Define the system message for the chatbot-style interaction.\n",
    "    system_message = (\n",
    "        \"You are a Vision Language Model specialized in interpreting visual data from chart images.\\n\"\n",
    "        \"Your task is to analyze the provided chart image and respond to queries with concise answers, \"\n",
    "        \"usually a single word, number, or short phrase.\\n\"\n",
    "        \"The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\\n\"\n",
    "        \"Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\n",
    "    )\n",
    "    \n",
    "    def format_data(sample):\n",
    "        # Ensure the label is in string format (if provided as a list, take the first element)\n",
    "        label_text = sample[\"label\"][0] if isinstance(sample[\"label\"], list) else sample[\"label\"]\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": sample[\"image\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": sample[\"query\"],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": label_text}],\n",
    "            },\n",
    "        ]\n",
    "    \n",
    "    # Format the train and evaluation datasets using the chatbot structure\n",
    "    train_dataset = [format_data(sample) for sample in raw_train_dataset]\n",
    "    eval_dataset = [format_data(sample) for sample in raw_eval_dataset]\n",
    "    \n",
    "    # ----------- Custom Collate Function (using tutorial's approach) -----------\n",
    "    def custom_collate_fn(examples):\n",
    "        # Apply the chat template to each example.\n",
    "        texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]\n",
    "        # Process the images using the helper function.\n",
    "        image_inputs = [process_vision_info(example)[0] for example in examples]\n",
    "        # Encode texts and images together.\n",
    "        batch = processor(\n",
    "            text=texts,\n",
    "            images=image_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=args.max_length\n",
    "        )\n",
    "        # Prepare labels by masking padding tokens.\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Mask image token indices using the tutorial-specified IDs.\n",
    "        image_tokens = [151652, 151653, 151655]\n",
    "        for token in image_tokens:\n",
    "            labels[labels == token] = -100\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    data_collator = custom_collate_fn\n",
    "\n",
    "    # Define separate directory for checkpoints\n",
    "    checkpoints_dir = os.path.join(args.model_dir, \"checkpoints\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    # Training arguments with DeepSpeed integration\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=checkpoints_dir,  # Checkpoints saved here,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=3e-7,\n",
    "        bf16=True,\n",
    "        logging_dir=os.path.join(args.output_data_dir, \"logs\"),\n",
    "        logging_steps=10,\n",
    "        log_level='debug',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        dataloader_num_workers=4,\n",
    "        deepspeed=deepspeed_config,\n",
    "        remove_unused_columns=False,  # Essential for multimodal training\n",
    "    )\n",
    "\n",
    "    # Custom callback to log GPU stats\n",
    "    class GPUStatsCallback(TrainerCallback):\n",
    "        def on_step_end(self, args, state, control, **kwargs):\n",
    "            gpu = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "            if gpu == -1:\n",
    "                return  # Skip if not using GPU\n",
    "            torch.cuda.synchronize(gpu)\n",
    "            allocated = torch.cuda.memory_allocated(gpu) / 1e9  # GB\n",
    "            reserved = torch.cuda.memory_reserved(gpu) / 1e9  # GB\n",
    "\n",
    "            try:\n",
    "                result = subprocess.check_output(\n",
    "                    ['nvidia-smi', '--id={}'.format(gpu), '--query-gpu=utilization.gpu,memory.used,memory.total', '--format=csv,nounits,noheader'],\n",
    "                    encoding='utf-8'\n",
    "                )\n",
    "                gpu_util, mem_used, mem_total = result.strip().split(',')\n",
    "                gpu_util = int(gpu_util)\n",
    "                mem_used = float(mem_used) / 1e3  # MB -> GB\n",
    "                mem_total = float(mem_total) / 1e3  # MB -> GB\n",
    "            except Exception as e:\n",
    "                gpu_util = 'N/A'\n",
    "                mem_used = allocated\n",
    "                mem_total = reserved\n",
    "                print(f\"Error getting GPU utilization: {e}\")\n",
    "\n",
    "            print(f\"After step {state.global_step}: GPU {gpu}, Utilization: {gpu_util}%, Memory Used: {mem_used:.2f} GB / {mem_total:.2f} GB\")\n",
    "    \n",
    "    # Initialize Trainer with DeepSpeed and callbacks\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[GPUStatsCallback()],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model - all processes must call this\n",
    "    print(\"Model trained successfully, proceeding to save...\")\n",
    "    trainer.save_model(args.model_dir)\n",
    "\n",
    "    # Only the main process handles tokenizer saving, model card creation, and cleanup\n",
    "    if trainer.is_world_process_zero():\n",
    "        print(\"Saving tokenizer and creating model card...\")\n",
    "        tokenizer.save_pretrained(args.model_dir)\n",
    "        trainer.create_model_card()\n",
    "\n",
    "        print(\"Saving completed. Verifying saved files...\")\n",
    "        list_dir_contents(args.model_dir)\n",
    "\n",
    "        try:\n",
    "            shutil.rmtree(checkpoints_dir)\n",
    "            print(f\"Removed checkpoints directory: {checkpoints_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing checkpoints directory: {e}\")\n",
    "        \n",
    "        list_dir_contents(args.model_dir)\n",
    "        print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-Create the requirements file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are inside of a docker container and cannot do many things, we need to install the necessary libraries through a single command therefore all the necessary libraries are added in this requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "transformers\n",
    "torch\n",
    "datasets\n",
    "accelerate\n",
    "sentencepiece\n",
    "bitsandbytes\n",
    "peft\n",
    "pyarrow\n",
    "deepspeed==0.15.4\n",
    "accelerate>=0.26.0\n",
    "pillow\n",
    "qwen_vl_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-Deepspeed Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since modeldataparallel and dataparallel are not enough to parallelizde this big models, we use deepspeed. Specifically Deepspeed ZeRO Stage 3. This allows us the maximum parallelization inside of the p4d and p4de clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** There are a few important things to take into account. Since deepspeed configuration is tied to the huggingface trainingArguments, make sure both definitions match (the one on step 3 and this one). If you are using p4d make sure that on the \"device\" definition is set to \"cpu\". You need to offload parameters and optimizers to cpu or your cluster will crash. If you have a p4de cluster, set it to \"none\" and the training time would be approximately half (1h vs 30mins for 7~8B family models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deepspeed_config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile deepspeed_config.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 1,\n",
    "  \"gradient_accumulation_steps\": 4,\n",
    "  \"steps_per_print\": 100,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": 1e-5,\n",
    "      \"betas\": [0.9, 0.999],\n",
    "      \"eps\": 1e-8,\n",
    "      \"weight_decay\": 3e-7\n",
    "    }\n",
    "  },\n",
    "  \"bf16\": {\n",
    "    \"enabled\": true\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"none\",\n",
    "      \"pin_memory\": true\n",
    "    },\n",
    "    \"offload_param\": {\n",
    "      \"device\": \"none\",\n",
    "      \"pin_memory\": true\n",
    "    },\n",
    "    \"allgather_partitions\": true,\n",
    "    \"allgather_bucket_size\": 50000000,\n",
    "    \"overlap_comm\": true,\n",
    "    \"reduce_scatter\": true,\n",
    "    \"reduce_bucket_size\": 50000000,\n",
    "    \"contiguous_gradients\": true\n",
    "  },\n",
    "  \"activation_checkpointing\": {\n",
    "    \"partition_activations\": true,\n",
    "    \"contiguous_memory_optimization\": true\n",
    "  },\n",
    "  \"wall_clock_breakdown\": false\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-Deepspeed Launcher**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unluckily we cannot launch deepspeed inside of our training job of step 3. Due to AWS and container related issues, if you try to do so, you wont be able to see the rest of the GPUs since your code is already running. In order to arrange the previous code, a deepspeed launcher must be made that directly from command line creates all the required processes (in this case 8 processes since we have 8 GPUs) and everything is coordinated by this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ds_launcher.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ds_launcher.py\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(\n",
    "        description=(\"SageMaker DeepSpeed Launch helper utility that will spawn deepspeed training scripts\")\n",
    "    )\n",
    "\n",
    "    # rest from the training program\n",
    "    parsed, nargs = parser.parse_known_args()\n",
    "\n",
    "    return nargs\n",
    "\n",
    "\n",
    "def main():\n",
    "    # https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/launcher/launch.py\n",
    "    num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))\n",
    "    hosts = json.loads(os.environ.get(\"SM_HOSTS\", \"{}\"))\n",
    "    num_nodes = len(hosts)\n",
    "    current_host = os.environ.get(\"SM_CURRENT_HOST\", 0)\n",
    "    rank = hosts.index(current_host)\n",
    "    print(f\"num_gpus = {num_gpus}, num_nodes = {num_nodes}, current_host = {current_host}, rank = {rank}\")\n",
    "\n",
    "    # os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "\n",
    "    # get number of GPU\n",
    "    # if num_gpus == 0:\n",
    "    #     raise ValueError(\"No GPUs found.\")\n",
    "\n",
    "    args = parse_args()\n",
    "    command = f\"deepspeed --num_gpus={num_gpus} train_deploy_huggingface.py {' '.join(args)}\"\n",
    "    print(f\"command = {command}\")\n",
    "    # launch deepspeed training\n",
    "    deepspeed_launch(command)\n",
    "\n",
    "\n",
    "def deepspeed_launch(command):\n",
    "    # try:\n",
    "    try:\n",
    "        subprocess.run(command, shell=True)\n",
    "    except Exception as e:\n",
    "        logger.info(e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6-Sagemaker Training Job Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step wraps up all of our work. Basically all files are passed here in order to be able to launch the codes inside of the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** Make sure that the instance type is set to the one you need (p4d or p4de) with the exact name AWS requires such as **ml.p4de.24xlarge** or **ml.p4d.24xlarge**. Also make sure all hyperparameters match with the previously described ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Replace with your actual Role ARN\n",
    "role = \"YOUR_AWS_ROLE\"\n",
    "\n",
    "distribution = {\n",
    "    \"deepspeed\": {\n",
    "        \"enabled\": True,\n",
    "        \"config_path\": \"deepspeed_config.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"ds_launcher.py\",\n",
    "    role=role,\n",
    "    source_dir=\".\",  # Ensure 'deepspeed_config.json' is included here\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4de.24xlarge\",  # Instance with 8 GPUs\n",
    "    framework_version=\"2.1.0\",  # Ensure compatibility with DeepSpeed\n",
    "    py_version=\"py310\",\n",
    "    dependencies=[\"requirements.txt\"],\n",
    "    hyperparameters={\n",
    "        \"epochs\": 4,\n",
    "        \"batch_size\": 1,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"max_length\": 4000,  # Updated max_length to 200\n",
    "        #Images Route (Is not a hyperparameter)\n",
    "    },\n",
    "    distribution=distribution,\n",
    "    output_path=f\"s3://{bucket_name}/{s3_prefix}/model\",\n",
    ")\n",
    "\n",
    "# Define S3 URIs for TSV data\n",
    "train_s3_uri = f\"s3://{bucket_name}/{train_s3_path}\"\n",
    "validation_s3_uri = f\"s3://{bucket_name}/{validation_s3_path}\"\n",
    "\n",
    "# Fit the model\n",
    "estimator.fit({\"train\": train_s3_uri, \"validation\": validation_s3_uri, \"images\": f\"s3://{bucket_name}/{s3_prefix}/images\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!explorer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/26/25 14:17:35] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file://C:\\Users\\user\\anaconda3\\lib\\site-packages\\botocore\\credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\user\\anaconda3\\lib\\site-packages\\botocore\\credentials.py#1352\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1352</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/26/25 14:17:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=241370;file://C:\\Users\\user\\anaconda3\\lib\\site-packages\\botocore\\credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=32363;file://C:\\Users\\user\\anaconda3\\lib\\site-packages\\botocore\\credentials.py#1352\u001b\\\u001b[2m1352\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x17ff73891b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os \n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=\"\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=\"\"\n",
    "os.environ['AWS_DEFAULT_REGION'] = \"us-west-2\"\n",
    "sagemaker.Session(boto3.session.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "035163cd4fbf43b0ae738becf470f3f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e42c0a1d56d4db0ba6823b21f2b8b16",
       "IPY_MODEL_4c8f47aa2ff94ead857fd2bd8b5d16a2",
       "IPY_MODEL_b9e51fdf9bc544f9a6f53ffa5e3beae9"
      ],
      "layout": "IPY_MODEL_f71a38aea2e5451896ea8daacbbf9120"
     }
    },
    "07a63b18f1764f38a55306dcfba1a4e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c8f47aa2ff94ead857fd2bd8b5d16a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9eac11e2d1c0472f8d685e57ef5fea85",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_769efd41f9bd43b48829f817d52cd117",
      "value": 4
     }
    },
    "6118e3106cff43ecab95b298412ba3e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ea2fb8b77694cd4a8ed2b64db3c4110": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "769efd41f9bd43b48829f817d52cd117": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8e42c0a1d56d4db0ba6823b21f2b8b16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dee7ecad8cb4fb5b6d02697b016db41",
      "placeholder": "​",
      "style": "IPY_MODEL_6ea2fb8b77694cd4a8ed2b64db3c4110",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "9dee7ecad8cb4fb5b6d02697b016db41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9eac11e2d1c0472f8d685e57ef5fea85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9e51fdf9bc544f9a6f53ffa5e3beae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07a63b18f1764f38a55306dcfba1a4e1",
      "placeholder": "​",
      "style": "IPY_MODEL_6118e3106cff43ecab95b298412ba3e6",
      "value": " 4/4 [00:11&lt;00:00,  2.44s/it]"
     }
    },
    "f71a38aea2e5451896ea8daacbbf9120": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
